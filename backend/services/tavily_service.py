import os
from typing import List, Dict, Any, Optional
from tavily import TavilyClient
import logging
from dotenv import load_dotenv
import asyncio

load_dotenv()

logger = logging.getLogger(__name__)

class TavilyAdvancedSearchService:
    """Ø®Ø¯Ù…Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Tavily Ù„Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ"""
    
    def __init__(self):
        self.api_key = os.environ.get('TAVILY_API_KEY')
        if not self.api_key:
            raise ValueError("TAVILY_API_KEY not found in environment variables")
        
        self.client = TavilyClient(api_key=self.api_key)
        
        # Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ®ØµØµØ© Ù„Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ
        self.search_domains = {
            'academic': [
                'jstor.org', 'academia.edu', 'researchgate.net',
                'journals.omanstudies.om', 'omanliterature.org'
            ],
            'official': [
                'moe.gov.om', 'heritage.gov.om', 'oman.om', 
                'omanobserver.om', 'omandaily.om', 'shabiba.com'
            ],
            'cultural': [
                'culturaloman.org', 'omanculture.net', 
                'omanpoetry.org', 'omaniheritage.com'
            ]
        }
    
    async def search_omani_literature_advanced(
        self, 
        query: str, 
        max_results: int = 5,
        include_domains: List[str] = None
    ) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø£Ø¯Ø¨ÙŠØ© Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø©"""
        
        try:
            # ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            enhanced_query = self._enhance_query_for_omani_literature(query)
            
            # Ø¨Ø­Ø« Ù…Ø®ØµØµ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµØ­ÙÙŠ ÙˆØ§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ù…Ù†Ø´ÙˆØ±
            search_response = self.client.search(
                query=enhanced_query,
                search_depth="advanced",  # Ø¨Ø­Ø« Ø¹Ù…ÙŠÙ‚
                max_results=max_results * 2,  # Ø¶Ø§Ø¹Ù Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØµÙÙŠØ©
                include_answer=True,
                include_domains=include_domains or self._get_journalism_domains(),
                exclude_domains=["facebook.com", "twitter.com", "instagram.com"],  # ØªØ¬Ù†Ø¨ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„
                include_raw_content=True,
                max_tokens=8000  # Ù…Ø­ØªÙˆÙ‰ Ø£Ø·ÙˆÙ„ Ù„Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            )
            
            # ØªØµÙÙŠØ© ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµØ­ÙÙŠ
            filtered_results = self._filter_for_journalism_content(search_response, query)
            
            return {
                'query': query,
                'enhanced_query': enhanced_query,
                'results': filtered_results,
                'total_found': len(filtered_results),
                'search_engine': 'tavily_journalism_focused',
                'answer_summary': search_response.get('answer', ''),
                'search_type': 'articles_and_interviews'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø­Ø« Tavily Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
            return {
                'query': query,
                'results': [],
                'total_found': 0,
                'error': str(e),
                'search_engine': 'tavily_journalism_error'
            }
    
    def _enhance_query_for_omani_literature(self, query: str) -> str:
        """ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø©"""
        
        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        interview_terms = [
            "Ù…Ù‚Ø§Ø¨Ù„Ø© Ù…Ø¹", "Ø­ÙˆØ§Ø± Ù…Ø¹", "Ù„Ù‚Ø§Ø¡ Ù…Ø¹", "interview with",
            "Ù…Ù‚Ø§Ù„ Ø¹Ù†", "ØªÙ‚Ø±ÙŠØ± Ø¹Ù†", "Ø¯Ø±Ø§Ø³Ø© Ø¹Ù†", "article about",
            "ØµØ­ÙŠÙØ©", "Ù…Ø¬Ù„Ø©", "Ù…ÙˆÙ‚Ø¹ Ø£Ø¯Ø¨ÙŠ", "Ù…Ù†Ø´ÙˆØ±"
        ]
        
        # Ù…ÙˆØ§Ù‚Ø¹ Ù…ÙˆØ«ÙˆÙ‚Ø© ØªÙ†Ø´Ø± Ù…Ù‚Ø§Ø¨Ù„Ø§Øª ÙˆÙ…Ù‚Ø§Ù„Ø§Øª Ø£Ø¯Ø¨ÙŠØ©
        reliable_sources = [
            "site:omanobserver.om", "site:omandaily.om", "site:shabiba.com",
            "site:alwatan.com", "site:omanalaan.com", "site:omansultanate.com",
            "site:moe.gov.om", "site:heritage.gov.om",
            "interview OR Ù…Ù‚Ø§Ø¨Ù„Ø© OR Ø­ÙˆØ§Ø± OR Ù„Ù‚Ø§Ø¡",
            "article OR Ù…Ù‚Ø§Ù„ OR ØªÙ‚Ø±ÙŠØ± OR Ø¯Ø±Ø§Ø³Ø©"
        ]
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©
        stop_words = ['Ø£Ø±ÙŠØ¯', 'Ø£Ø¹Ø·Ù†ÙŠ', 'Ù‚Ù„ Ù„ÙŠ', 'Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù†']
        cleaned_query = query
        for word in stop_words:
            cleaned_query = cleaned_query.replace(word, '')
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø­Ø³Ù† ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù†Ø´ÙˆØ±
        enhanced = f'{cleaned_query} Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ ("Ù…Ù‚Ø§Ø¨Ù„Ø©" OR "Ø­ÙˆØ§Ø±" OR "Ù…Ù‚Ø§Ù„" OR "interview" OR "article")'
        
        # Ø¥Ø¶Ø§ÙØ© Ù…ØµØ·Ù„Ø­Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ØµØ§Ø¯Ø± ØµØ­ÙÙŠØ© ÙˆØ£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ù…ÙˆØ«ÙˆÙ‚Ø©
        enhanced += f' ({" OR ".join(reliable_sources[:5])})'
        
        logger.info(f"Ø§Ø³ØªØ¹Ù„Ø§Ù… Tavily Ù…Ø­Ø³Ù†: {enhanced}")
        
        return enhanced.strip()
    
    def _get_priority_domains(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø¨Ø­Ø«"""
        priority_domains = []
        
        # Ø¬Ù…Ø¹ Ø£Ù‡Ù… Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
        for category, domains in self.search_domains.items():
            priority_domains.extend(domains[:2])  # Ø£ÙˆÙ„ Ù†Ø·Ø§Ù‚ÙŠÙ† Ù…Ù† ÙƒÙ„ ÙØ¦Ø©
        
        # Ø¥Ø¶Ø§ÙØ© Ù†Ø·Ø§Ù‚Ø§Øª Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø¹Ø§Ù…Ø©
        priority_domains.extend([
            'wikipedia.org', 'britannica.com', 'scholar.google.com'
        ])
        
        return priority_domains
    
    def _get_journalism_domains(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„ØµØ­Ø§ÙØ© ÙˆØ§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø£Ø¯Ø¨ÙŠØ©"""
        journalism_domains = []
        
        # Ù…ÙˆØ§Ù‚Ø¹ ØµØ­ÙÙŠØ© Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ù…ÙˆØ«ÙˆÙ‚Ø©
        journalism_domains.extend([
            'omanobserver.om', 'omandaily.om', 'shabiba.com',
            'alwatan.com', 'omanalaan.com', 'omansultanate.com',
            'timesofoman.com', 'muscatdaily.com'
        ])
        
        # Ù…ÙˆØ§Ù‚Ø¹ Ø­ÙƒÙˆÙ…ÙŠØ© Ø«Ù‚Ø§ÙÙŠØ©
        journalism_domains.extend([
            'moe.gov.om', 'heritage.gov.om', 'oman.om'
        ])
        
        # Ù…ÙˆØ§Ù‚Ø¹ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© ÙˆÙ…Ø¬Ù„Ø§Øª Ø£Ø¯Ø¨ÙŠØ©
        journalism_domains.extend([
            'jstor.org', 'academia.edu', 'researchgate.net',
            'journals.omanstudies.om', 'omanliterature.org'
        ])
        
        return journalism_domains
    
    def _process_tavily_results(
        self, 
        search_response: Dict[str, Any], 
        original_query: str
    ) -> List[Dict[str, Any]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØµÙÙŠØ© Ù†ØªØ§Ø¦Ø¬ Tavily"""
        
        raw_results = search_response.get('results', [])
        processed_results = []
        
        for result in raw_results:
            # ØªÙ‚ÙŠÙŠÙ… ØµÙ„Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ
            relevance_score = self._calculate_relevance_score(result, original_query)
            
            if relevance_score > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„ØµÙ„Ø©
                processed_result = {
                    'title': result.get('title', ''),
                    'content': result.get('content', '')[:600],  # Ø£ÙˆÙ„ 600 Ø­Ø±Ù
                    'url': result.get('url', ''),
                    'score': result.get('score', 0),
                    'relevance_score': relevance_score,
                    'source_type': self._identify_source_type(result.get('url', '')),
                    'reliability_rating': self._rate_source_reliability(result.get('url', '')),
                    'published_date': result.get('published_date'),
                    'raw_content': result.get('raw_content', '')[:200]  # Ù…Ø­ØªÙˆÙ‰ Ø®Ø§Ù… Ù‚ØµÙŠØ±
                }
                
                # Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                processed_result['omani_keywords'] = self._extract_omani_keywords(
                    processed_result['content']
                )
                
                processed_results.append(processed_result)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø© ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        processed_results.sort(
            key=lambda x: (x['relevance_score'], x['reliability_rating']), 
            reverse=True
        )
        
        return processed_results
    
    def _calculate_relevance_score(self, result: Dict[str, Any], query: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØµÙ„Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ"""
        
        content = (result.get('content', '') + ' ' + result.get('title', '')).lower()
        query_lower = query.lower()
        
        score = 0.0
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠØ©
        omani_keywords = ['Ø¹ÙÙ…Ø§Ù†', 'Ø¹ÙÙ…Ø§Ù†ÙŠ', 'oman', 'omani', 'Ø³Ù„Ø·Ù†Ø©']
        for keyword in omani_keywords:
            if keyword.lower() in content:
                score += 0.3
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø£Ø¯Ø¨ÙŠØ©
        literary_terms = ['Ø£Ø¯Ø¨', 'Ø´Ø¹Ø±', 'Ø±ÙˆØ§ÙŠØ©', 'Ù‚ØµØ©', 'Ø´Ø§Ø¹Ø±', 'literature', 'poetry']
        for term in literary_terms:
            if term.lower() in content:
                score += 0.2
        
        # Ù†Ù‚Ø§Ø· Ù„ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query_words = query_lower.split()
        for word in query_words:
            if len(word) > 2 and word in content:
                score += 0.1
        
        return min(1.0, score)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 1.0
    
    def _identify_source_type(self, url: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø± Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        url_lower = url.lower()
        
        if '.edu' in url_lower or 'academic' in url_lower or 'journal' in url_lower:
            return 'Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ'
        elif '.gov.om' in url_lower or 'official' in url_lower:
            return 'Ø­ÙƒÙˆÙ…ÙŠ Ø±Ø³Ù…ÙŠ'
        elif 'wikipedia' in url_lower:
            return 'Ù…ÙˆØ³ÙˆØ¹ÙŠ'
        elif 'news' in url_lower or 'observer' in url_lower:
            return 'ØµØ­ÙÙŠ'
        else:
            return 'Ø¹Ø§Ù…'
    
    def _rate_source_reliability(self, url: str) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„Ù…ØµØ¯Ø±"""
        url_lower = url.lower()
        
        # Ù…ØµØ§Ø¯Ø± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        if any(domain in url_lower for domain in ['.edu', '.gov.om', 'jstor', 'academia.edu']):
            return 0.9
        
        # Ù…ØµØ§Ø¯Ø± Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        elif any(domain in url_lower for domain in ['wikipedia', 'britannica', 'observer']):
            return 0.7
        
        # Ù…ØµØ§Ø¯Ø± Ù…Ù†Ø®ÙØ¶Ø© Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        else:
            return 0.5
    
    def _extract_omani_keywords(self, content: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        
        omani_specific_terms = [
            'Ø¹ÙÙ…Ø§Ù†', 'Ø¹ÙÙ…Ø§Ù†ÙŠ', 'Ø¹ÙÙ…Ø§Ù†ÙŠØ©', 'Ù…Ø³Ù‚Ø·', 'ØµÙ„Ø§Ù„Ø©', 'Ù†Ø²ÙˆÙ‰', 'ØµØ­Ø§Ø±',
            'Ø§Ù„Ø¨Ø§Ø·Ù†Ø©', 'Ø¸ÙØ§Ø±', 'Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©', 'Ù…Ø³Ù†Ø¯Ù…', 'Ø§Ù„ÙˆØ³Ø·Ù‰',
            'Ø³Ù„Ø·Ù†Ø©', 'Ø³Ù„Ø·Ø§Ù†', 'Ù‚Ø§Ø¨ÙˆØ³', 'Ù‡ÙŠØ«Ù…', 'Ø¢Ù„ Ø³Ø¹ÙŠØ¯',
            'Ø®Ù†Ø¬Ø±', 'Ø¹Ù†Ø²Ø©', 'Ù‡Ø¬Ù†', 'Ù†Ø®ÙŠÙ„', 'Ù„Ø¨Ø§Ù†', 'Ø¨Ø®ÙˆØ±',
            'ÙØ±Ù‚Ø©', 'Ø±Ø²Ø­Ø©', 'Ø¨Ø±Ø¹Ø©', 'ÙÙ†', 'ØªØ±Ø§Ø«'
        ]
        
        found_keywords = []
        content_lower = content.lower()
        
        for term in omani_specific_terms:
            if term.lower() in content_lower:
                found_keywords.append(term)
        
        return found_keywords
    
    async def search_specific_author(self, author_name: str) -> Dict[str, Any]:
        """Ø¨Ø­Ø« Ù…ØªØ®ØµØµ Ø¹Ù† Ù…Ø¤Ù„Ù Ø¹ÙÙ…Ø§Ù†ÙŠ Ù…Ø¹ÙŠÙ†"""
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø®ØµØµ Ù„Ù„Ù…Ø¤Ù„ÙÙŠÙ†
        author_query = f'"{author_name}" Ø´Ø§Ø¹Ø± Ø¹ÙÙ…Ø§Ù†ÙŠ OR "{author_name}" Ø£Ø¯ÙŠØ¨ Ø¹ÙÙ…Ø§Ù†ÙŠ OR "{author_name}" Omani poet OR "{author_name}" Omani writer'
        
        return await self.search_omani_literature_advanced(
            query=author_query,
            max_results=7,
            include_domains=self.search_domains['academic'] + self.search_domains['official']
        )
    
    async def search_literary_work(self, work_title: str, author: str = "") -> Dict[str, Any]:
        """Ø¨Ø­Ø« Ù…ØªØ®ØµØµ Ø¹Ù† Ø¹Ù…Ù„ Ø£Ø¯Ø¨ÙŠ Ù…Ø¹ÙŠÙ†"""
        
        work_query = f'"{work_title}" {author} Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ'
        if author:
            work_query += f' "{author}"'
        
        work_query += ' Ø±ÙˆØ§ÙŠØ© OR Ù‚ØµÙŠØ¯Ø© OR Ø¯ÙŠÙˆØ§Ù† OR Ù…Ø³Ø±Ø­ÙŠØ© OR Ù‚ØµØ©'
        
        return await self.search_omani_literature_advanced(
            query=work_query,
            max_results=5
        )
    
    def format_results_for_llm(self, search_results: Dict[str, Any]) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        
        if not search_results.get('results'):
            return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø©."
        
        formatted = f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù€: {search_results['query']}\n\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù„Ø®Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
        if search_results.get('answer_summary'):
            formatted += f"Ø§Ù„Ù…Ù„Ø®Øµ: {search_results['answer_summary']}\n\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        for i, result in enumerate(search_results['results'], 1):
            reliability_emoji = "ğŸ†" if result['reliability_rating'] > 0.8 else "âœ…" if result['reliability_rating'] > 0.6 else "âš ï¸"
            
            formatted += f"{i}. {result['title']} {reliability_emoji}\n"
            formatted += f"Ø§Ù„Ù†ÙˆØ¹: {result['source_type']} | Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©: {result['reliability_rating']:.1f}/1.0\n"
            formatted += f"Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {result['content'][:300]}...\n"
            
            if result['omani_keywords']:
                formatted += f"ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¹ÙÙ…Ø§Ù†ÙŠØ©: {', '.join(result['omani_keywords'][:5])}\n"
            
            formatted += f"Ø§Ù„Ù…ØµØ¯Ø±: {result['url']}\n\n"
        
        formatted += f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {search_results['total_found']} | Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø«: Tavily Advanced"
        
        return formatted

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø®Ø¯Ù…Ø© Tavily
tavily_search_service = TavilyAdvancedSearchService()