from typing import List, Dict, Any, Optional
import logging
from datetime import datetime

from models.literature_models import Author, LiteraryWork, AcademicSource
from services.embeddings_service import EmbeddingsService
from services.academic_collector import academic_collector
from services.tavily_service import tavily_search_service

logger = logging.getLogger(__name__)

class AdvancedRAGService:
    """Ù†Ø¸Ø§Ù… RAG Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ"""
    
    def __init__(self, db):
        self.db = db
        self.embeddings_service = EmbeddingsService(db)
        
        # Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.authors_collection = db.authors
        self.works_collection = db.literary_works
        self.sources_collection = db.academic_sources
        self.queries_collection = db.search_queries
    
    async def comprehensive_search_and_answer(
        self, 
        user_query: str, 
        session_id: str
    ) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        
        try:
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            await self._log_search_query(user_query, session_id)
            
            # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_analysis = self._analyze_query_type(user_query)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±
            search_results = await self._multi_source_search(user_query, query_analysis)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
            semantic_results = await self.embeddings_service.semantic_search(
                user_query, 
                limit=5
            )
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ±ØªÙŠØ¨Ù‡Ø§
            combined_results = self._merge_and_rank_results(
                search_results, 
                semantic_results,
                query_analysis
            )
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
            context = self._prepare_rag_context(combined_results, query_analysis)
            
            return {
                'query': user_query,
                'query_type': query_analysis['type'],
                'context': context,
                'sources_found': len(combined_results),
                'semantic_matches': len(semantic_results),
                'external_sources': len(search_results.get('results', [])),
                'confidence_level': self._calculate_context_confidence(combined_results)
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {e}")
            return {
                'query': user_query,
                'context': "",
                'error': str(e),
                'sources_found': 0
            }
    
    def _analyze_query_type(self, query: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«"""
        
        query_lower = query.lower()
        
        analysis = {
            'type': 'general',
            'target_author': None,
            'target_work': None,
            'analysis_needed': False,
            'search_priority': 'balanced'
        }
        
        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ù…Ø¤Ù„Ù Ù…Ø¹ÙŠÙ†
        for author in academic_collector.known_omani_authors:
            if author.lower() in query_lower:
                analysis['type'] = 'author_specific'
                analysis['target_author'] = author
                analysis['search_priority'] = 'author_focused'
                break
        
        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø¨ÙŠ
        analytical_keywords = ['ØªØ­Ù„ÙŠÙ„', 'Ù†Ù‚Ø¯', 'Ø¥Ø¹Ø±Ø§Ø¨', 'Ø¨Ù„Ø§ØºØ©', 'Ø£Ø³Ù„ÙˆØ¨']
        if any(keyword in query_lower for keyword in analytical_keywords):
            analysis['analysis_needed'] = True
            analysis['search_priority'] = 'analytical'
        
        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ù…Ø¤Ù„ÙØ§Øª Ù…Ø­Ø¯Ø¯Ø©
        work_keywords = ['Ù…Ø¤Ù„ÙØ§Øª', 'Ø£Ø¹Ù…Ø§Ù„', 'ÙƒØªØ¨', 'Ø¯ÙˆØ§ÙˆÙŠÙ†', 'Ø±ÙˆØ§ÙŠØ§Øª']
        if any(keyword in query_lower for keyword in work_keywords):
            analysis['type'] = 'works_inquiry'
            analysis['search_priority'] = 'works_focused'
        
        return analysis
    
    async def _multi_source_search(
        self, 
        query: str, 
        query_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        
        search_results = {'results': [], 'sources': []}
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ Ø¨Ù€ Tavily
            if query_analysis['target_author']:
                # Ø¨Ø­Ø« Ù…Ø®ØµØµ Ù„Ù„Ù…Ø¤Ù„Ù
                tavily_results = await tavily_search_service.search_specific_author(
                    query_analysis['target_author']
                )
            else:
                # Ø¨Ø­Ø« Ø¹Ø§Ù…
                tavily_results = await tavily_search_service.search_omani_literature_advanced(query)
            
            search_results['results'] = tavily_results.get('results', [])
            search_results['sources'].append('tavily_advanced')
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
            local_results = await self._search_local_knowledge(query, query_analysis)
            if local_results:
                search_results['results'].extend(local_results)
                search_results['sources'].append('local_knowledge')
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±: {e}")
        
        return search_results
    
    async def _search_local_knowledge(
        self, 
        query: str, 
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
        
        local_results = []
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¤Ù„ÙÙŠÙ†
            if query_analysis['type'] in ['author_specific', 'general']:
                authors_cursor = self.authors_collection.find({
                    '$text': {'$search': query}
                }).limit(3)
                
                async for author_doc in authors_cursor:
                    local_results.append({
                        'title': f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† {author_doc.get('full_name', 'Ù…Ø¤Ù„Ù')}",
                        'content': author_doc.get('biography', ''),
                        'source_type': 'local_author',
                        'reliability_rating': 0.95,  # Ø¹Ø§Ù„ÙŠØ© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ù…Ø­Ù‚Ù‚
                        'metadata': {
                            'author_id': author_doc.get('id'),
                            'genres': author_doc.get('main_genres', [])
                        }
                    })
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø£Ø¯Ø¨ÙŠØ©
            if query_analysis['type'] in ['works_inquiry', 'general']:
                works_cursor = self.works_collection.find({
                    '$text': {'$search': query}
                }).limit(3)
                
                async for work_doc in works_cursor:
                    local_results.append({
                        'title': work_doc.get('title', 'Ø¹Ù…Ù„ Ø£Ø¯Ø¨ÙŠ'),
                        'content': work_doc.get('summary', ''),
                        'source_type': 'local_work',
                        'reliability_rating': 0.95,
                        'metadata': {
                            'work_id': work_doc.get('id'),
                            'category': work_doc.get('category'),
                            'style': work_doc.get('style')
                        }
                    })
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ: {e}")
        
        return local_results
    
    def _merge_and_rank_results(
        self, 
        external_results: Dict[str, Any],
        semantic_results: List[Dict[str, Any]],
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Ø¯Ù…Ø¬ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©"""
        
        combined = []
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
        for result in external_results.get('results', []):
            result['result_source'] = 'external'
            result['final_score'] = self._calculate_result_score(result, query_analysis)
            combined.append(result)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
        for result in semantic_results:
            result['result_source'] = 'semantic'
            result['final_score'] = result['similarity_score'] * 1.2  # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø­Ù„ÙŠ
            combined.append(result)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        combined.sort(key=lambda x: x.get('final_score', 0), reverse=True)
        
        return combined[:8]  # Ø£ÙØ¶Ù„ 8 Ù†ØªØ§Ø¦Ø¬
    
    def _calculate_result_score(
        self, 
        result: Dict[str, Any], 
        query_analysis: Dict[str, Any]
    ) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†ØªÙŠØ¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù†ØªÙŠØ¬Ø©"""
        
        base_score = result.get('reliability_rating', 0.5)
        
        # Ù…ÙƒØ§ÙØ¢Øª Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        if query_analysis['type'] == 'author_specific':
            if result.get('source_type') in ['interview', 'local_author']:
                base_score += 0.3
        
        elif query_analysis['type'] == 'works_inquiry':
            if result.get('source_type') in ['book_metadata', 'local_work']:
                base_score += 0.3
        
        # Ù…ÙƒØ§ÙØ£Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ù…Ø­Ù‚Ù‚
        if result.get('result_source') == 'semantic':
            base_score += 0.2
        
        return min(1.0, base_score)
    
    def _prepare_rag_context(
        self, 
        results: List[Dict[str, Any]], 
        query_analysis: Dict[str, Any]
    ) -> str:
        """ØªØ­Ø¶ÙŠØ± Ø³ÙŠØ§Ù‚ RAG Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        
        if not results:
            return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."
        
        context = f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ (Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query_analysis['type']}):\n\n"
        
        high_confidence_results = [r for r in results if r.get('final_score', 0) > 0.7]
        medium_confidence_results = [r for r in results if 0.4 <= r.get('final_score', 0) <= 0.7]
        
        if high_confidence_results:
            context += "--- Ù…ØµØ§Ø¯Ø± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø© ---\n"
            for i, result in enumerate(high_confidence_results[:4], 1):
                source_emoji = "ğŸ“š" if result.get('result_source') == 'semantic' else "ğŸŒ"
                context += f"{i}. {source_emoji} {result.get('title', 'Ø¨Ù„Ø§ Ø¹Ù†ÙˆØ§Ù†')}\n"
                context += f"Ø§Ù„Ù†ÙˆØ¹: {result.get('source_type', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}\n"
                context += f"Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {result.get('content', '')[:300]}...\n"
                context += f"Ø§Ù„Ø«Ù‚Ø©: {result.get('final_score', 0):.2f}/1.0\n\n"
        
        if medium_confidence_results:
            context += "--- Ù…ØµØ§Ø¯Ø± Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ø«Ù‚Ø© (Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ø³) ---\n"
            for i, result in enumerate(medium_confidence_results[:2], 1):
                context += f"{i}. {result.get('title', 'Ø¨Ù„Ø§ Ø¹Ù†ÙˆØ§Ù†')}\n"
                context += f"Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {result.get('content', '')[:150]}...\n\n"
        
        context += f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø±: {len(results)} | Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: Ù…ØªØ§Ø­"
        
        return context
    
    def _calculate_context_confidence(self, results: List[Dict[str, Any]]) -> str:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ÙØ¬Ù…Ø¹"""
        
        if not results:
            return "Ù…Ù†Ø®ÙØ¶"
        
        avg_score = sum(r.get('final_score', 0) for r in results) / len(results)
        
        if avg_score >= 0.8:
            return "Ø¹Ø§Ù„Ù"
        elif avg_score >= 0.6:
            return "Ù…ØªÙˆØ³Ø·"
        else:
            return "Ù…Ù†Ø®ÙØ¶"
    
    async def _log_search_query(self, query: str, session_id: str):
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
        try:
            query_record = {
                'query_text': query,
                'user_session': session_id,
                'timestamp': datetime.utcnow(),
                'search_type': self._analyze_query_type(query)['type']
            }
            
            await self.queries_collection.insert_one(query_record)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
    
    async def add_verified_author(self, author_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ù„Ù Ù…Ø­Ù‚Ù‚ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¬Ù„ Ù…Ø¤Ù„Ù
            author = Author(**author_data)
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await self.authors_collection.insert_one(author.dict())
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡
            embedding_id = await self.embeddings_service.create_author_embedding(author)
            
            logger.info(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ù„Ù Ù…Ø­Ù‚Ù‚: {author.full_name}")
            
            return {
                'success': True,
                'author_id': author.id,
                'embedding_id': embedding_id,
                'message': f'ØªÙ… Ø¥Ø¶Ø§ÙØ© {author.full_name} Ø¨Ù†Ø¬Ø§Ø­'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ù„Ù: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def add_verified_work(self, work_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ø¥Ø¶Ø§ÙØ© Ø¹Ù…Ù„ Ø£Ø¯Ø¨ÙŠ Ù…Ø­Ù‚Ù‚ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø£Ø¯Ø¨ÙŠ
            work = LiteraryWork(**work_data)
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await self.works_collection.insert_one(work.dict())
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡
            embedding_id = await self.embeddings_service.create_work_embedding(work)
            
            logger.info(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹Ù…Ù„ Ù…Ø­Ù‚Ù‚: {work.title}")
            
            return {
                'success': True,
                'work_id': work.id,
                'embedding_id': embedding_id,
                'message': f'ØªÙ… Ø¥Ø¶Ø§ÙØ© {work.title} Ø¨Ù†Ø¬Ø§Ø­'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù…Ù„: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def auto_collect_and_process(self) -> Dict[str, Any]:
        """Ø¬Ù…Ø¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù„Ù…ØµØ§Ø¯Ø±"""
        try:
            logger.info("Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ù…Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©...")
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø´Ø§Ù…Ù„Ø©
            collection_results = await academic_collector.collect_comprehensive_sources()
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆÙ…Ø¹Ø¯Ø§ØªØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            processing_stats = await self._process_collected_sources(collection_results)
            
            return {
                'collection_completed': True,
                'sources_collected': collection_results['total_collected'],
                'sources_processed': processing_stats['processed_count'],
                'embeddings_created': processing_stats['embeddings_count'],
                'completion_date': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¬Ù…Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {e}")
            return {
                'collection_completed': False,
                'error': str(e)
            }
    
    async def _process_collected_sources(self, collection_results: Dict[str, Any]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ù†Ø¸Ù…Ø©"""
        
        processing_stats = {
            'processed_count': 0,
            'embeddings_count': 0,
            'errors': []
        }
        
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¨Ø­Ø§Ø« Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©
            for paper in collection_results.get('academic_papers', []):
                try:
                    academic_source = AcademicSource(
                        title=paper['title'],
                        authors=[paper.get('covered_author', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')],
                        abstract=paper['abstract'],
                        url=paper.get('url'),
                        topic="Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ",
                        covered_authors=[paper.get('covered_author', '')],
                        relevance_score=paper.get('reliability_score', 0.8)
                    )
                    
                    await self.sources_collection.insert_one(academic_source.dict())
                    processing_stats['processed_count'] += 1
                    
                except Exception as e:
                    processing_stats['errors'].append(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø­Ø«: {e}")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª
            for interview in collection_results.get('interviews', []):
                try:
                    # Ø¥Ø¶Ø§ÙØ© ÙƒÙ…ØµØ¯Ø± Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ
                    interview_source = AcademicSource(
                        title=interview['title'],
                        authors=[interview.get('interviewee', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')],
                        abstract=interview['content'][:500],
                        full_text=interview.get('content'),
                        url=interview.get('url'),
                        topic=f"Ù…Ù‚Ø§Ø¨Ù„Ø© Ù…Ø¹ {interview.get('interviewee', 'Ø£Ø¯ÙŠØ¨ Ø¹ÙÙ…Ø§Ù†ÙŠ')}",
                        covered_authors=[interview.get('interviewee', '')],
                        relevance_score=interview.get('reliability_score', 0.7)
                    )
                    
                    await self.sources_collection.insert_one(interview_source.dict())
                    processing_stats['processed_count'] += 1
                    
                except Exception as e:
                    processing_stats['errors'].append(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©: {e}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±: {e}")
            processing_stats['errors'].append(str(e))
        
        return processing_stats
    
    async def get_rag_statistics(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ø¸Ø§Ù… RAG"""
        try:
            stats = {
                'authors_count': await self.authors_collection.count_documents({}),
                'works_count': await self.works_collection.count_documents({}),
                'sources_count': await self.sources_collection.count_documents({}),
                'queries_count': await self.queries_collection.count_documents({}),
            }
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
            embeddings_stats = await self.embeddings_service.get_embeddings_stats()
            stats['embeddings'] = embeddings_stats
            
            # Ø¢Ø®Ø± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
            recent_queries_cursor = self.queries_collection.find().sort('timestamp', -1).limit(5)
            recent_queries = await recent_queries_cursor.to_list(5)
            stats['recent_queries'] = [q.get('query_text', '') for q in recent_queries]
            
            return stats
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª RAG: {e}")
            return {'error': str(e)}

# Ø³ÙŠØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
rag_service = None