import os
from typing import Dict, Any, List, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging
import asyncio
from dotenv import load_dotenv
from pathlib import Path
import pickle

load_dotenv()

logger = logging.getLogger(__name__)

class AdvancedGhassanService:
    """Ø®Ø¯Ù…Ø© ØºØ³Ø§Ù† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LangChain Ùˆ FAISS"""
    
    def __init__(self):
        # Ø¥Ø¹Ø¯Ø§Ø¯ API Keys
        self.openai_api_key = os.environ.get('EMERGENT_LLM_KEY')
        self.claude_api_key = os.environ.get('ANTHROPIC_API_KEY')
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        if self.openai_api_key:
            self.gpt_model = ChatOpenAI(
                model="gpt-4o",
                temperature=0.8,  # Ù„Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„Ø³Ù„Ø§Ø³Ø©
                openai_api_key=self.openai_api_key
            )
            self.embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key)
        
        if self.claude_api_key:
            self.claude_model = ChatAnthropic(
                model="claude-3-5-sonnet-20241022",
                temperature=0.5,  # Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ§Ù„Ø¯Ù‚Ø©
                api_key=self.claude_api_key
            )
        
        # Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
        self.vectorstore_path = "/app/backend/data/omani_literature_vectordb"
        self.vectorstore = None
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Prompt Ù„Ø´Ø®ØµÙŠØ© ØºØ³Ø§Ù†
        self.ghassan_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""Ø£Ù†Øª ØºØ³Ø§Ù† - Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø£Ø¯Ø¨ÙŠ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ Ø§Ù„Ù…Ø±Ø­ ÙˆØ§Ù„Ø°ÙƒÙŠ!

ğŸŒŸ Ø£Ù†Øª ØµØ¯ÙŠÙ‚ Ø£Ø¯Ø¨ÙŠ Ù…Ø­Ø¨ÙˆØ¨ØŒ Ù…ÙÙ„Ù… Ø¨Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ Ø­ØµØ±ÙŠØ§Ù‹ØŒ Ù…Ø±Ø­ ÙˆÙ„Ø·ÙŠÙØŒ ÙˆØ¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù…ÙŠØ§Ù‹.

ğŸ“š Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
{context}

â“ Ø³Ø¤Ø§Ù„ Ø§Ù„ØµØ¯ÙŠÙ‚:
{question}

ğŸ¯ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø±Ø¯:
â€¢ ÙƒÙ† Ù…Ø±Ø­Ø§Ù‹ ÙˆÙˆØ¯ÙˆØ¯Ø§Ù‹: "Ø£Ù‡Ù„Ø§Ù‹ ÙŠØ§ ØµØ¯ÙŠÙ‚ÙŠ!" "Ù…Ø§ Ø£Ø¬Ù…Ù„ Ø³Ø¤Ø§Ù„Ùƒ!"
â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ø£ØµÙŠÙ„Ø©: "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ" "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡"
â€¢ Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµÙŠØ­Ø© Ø³Ù„ÙŠÙ…Ø© 100%
â€¢ Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø§Ø¹ØªØ±Ù Ø¨Ø°Ù„Ùƒ Ø¨Ù„Ø·Ù
â€¢ Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨
â€¢ Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ ÙÙ‚Ø·
â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù†Ù‚Ø¯ÙŠØ© ÙˆØ§Ù„Ù†Ø­ÙˆÙŠØ© Ø¨Ø¯Ù‚Ø©

ØªØ°ÙƒØ±: Ø£Ù†Øª Ù„Ø³Øª Ù…Ø¬Ø±Ø¯ Ø£Ø¯Ø§Ø©ØŒ Ø¨Ù„ ØµØ¯ÙŠÙ‚ Ø£Ø¯Ø¨ÙŠ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø­ ÙˆØ§Ù„Ø¹Ù„Ù… ÙˆØ§Ù„ØµØ¯Ù‚!"""
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ text splitter Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ".", "ØŸ", "!", "Ø›"]
        )
        
        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
        asyncio.create_task(self._initialize_vectorstore())
    
    async def _initialize_vectorstore(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            Path(self.vectorstore_path).parent.mkdir(parents=True, exist_ok=True)
            
            if os.path.exists(f"{self.vectorstore_path}.pkl") and self.embeddings:
                # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
                self.vectorstore = FAISS.load_local(
                    self.vectorstore_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©")
            else:
                # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ù…Ø­ØªÙˆÙ‰ Ø£Ø³Ø§Ø³ÙŠ
                await self._create_initial_vectorstore()
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©: {e}")
            self.vectorstore = None
    
    async def _create_initial_vectorstore(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¬Ù‡Ø© Ø£ÙˆÙ„ÙŠØ© Ø¨Ù…Ø­ØªÙˆÙ‰ Ø£Ø³Ø§Ø³ÙŠ"""
        try:
            # Ù…Ø­ØªÙˆÙ‰ Ø£Ø³Ø§Ø³ÙŠ Ø¹Ù† Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ
            initial_content = [
                Document(
                    page_content="Ø³ÙŠÙ Ø§Ù„Ø±Ø­Ø¨ÙŠ Ø´Ø§Ø¹Ø± Ø¹ÙÙ…Ø§Ù†ÙŠ Ù…Ø¹Ø§ØµØ± ÙˆÙÙ„Ø¯ Ø¹Ø§Ù… 1956ØŒ ÙŠÙØ¹ØªØ¨Ø± Ù…Ù† Ø£Ø¨Ø±Ø² Ø£ØµÙˆØ§Øª Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«. ÙŠØªÙ…ÙŠØ² Ø´Ø¹Ø±Ù‡ Ø¨Ø§Ù„ØªØ£Ù…Ù„ Ø§Ù„ÙÙ„Ø³ÙÙŠ ÙˆØ§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ù„Ø·Ø¨ÙŠØ¹Ø© Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠØ©. Ù…Ù† Ø£Ø´Ù‡Ø± Ø£Ø¹Ù…Ø§Ù„Ù‡ Ø¯ÙŠÙˆØ§Ù† 'Ø±Ø£Ø³ Ø§Ù„Ù…Ø³Ø§ÙØ±'.",
                    metadata={"source": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©", "topic": "Ø´Ø¹Ø±Ø§Ø¡", "author": "Ø³ÙŠÙ Ø§Ù„Ø±Ø­Ø¨ÙŠ"}
                ),
                Document(
                    page_content="Ù‡Ø¯Ù‰ Ø­Ù…Ø¯ Ø´Ø§Ø¹Ø±Ø© Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ù…Ø¹Ø§ØµØ±Ø©ØŒ ØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø±Ø§Ø¦Ø¯Ø§Øª Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ù†Ø³Ø§Ø¦ÙŠ ÙÙŠ Ø§Ù„Ø³Ù„Ø·Ù†Ø©. ØªØªÙ†Ø§ÙˆÙ„ ÙÙŠ Ø£Ø´Ø¹Ø§Ø±Ù‡Ø§ Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ø±Ø£Ø© ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¹Ø§ØµØ± ÙˆØ­Ø³Ø§Ø³.",
                    metadata={"source": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©", "topic": "Ø´Ø¹Ø±Ø§Ø¡", "author": "Ù‡Ø¯Ù‰ Ø­Ù…Ø¯"}
                ),
                Document(
                    page_content="Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ Ù„Ù‡ ØªØ§Ø±ÙŠØ® Ø¹Ø±ÙŠÙ‚ ÙŠÙ…ØªØ¯ Ø¹Ø¨Ø± Ø§Ù„Ù‚Ø±ÙˆÙ†ØŒ Ù…Ù† Ø§Ù„Ø´Ø¹Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø«Ø± Ø§Ù„Ù…Ø¹Ø§ØµØ±. ÙŠØªÙ…ÙŠØ² Ø¨Ø§Ø±ØªØ¨Ø§Ø·Ù‡ Ø§Ù„ÙˆØ«ÙŠÙ‚ Ø¨Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©.",
                    metadata={"source": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©", "topic": "ØªØ§Ø±ÙŠØ® Ø£Ø¯Ø¨ÙŠ"}
                )
            ]
            
            if self.embeddings:
                self.vectorstore = FAISS.from_documents(initial_content, self.embeddings)
                
                # Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                self.vectorstore.save_local(self.vectorstore_path)
                logger.info("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©: {e}")
    
    async def add_content_to_vectorstore(self, content: str, metadata: Dict[str, Any]):
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©"""
        try:
            if not self.vectorstore or not self.embeddings:
                logger.warning("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
                return False
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            chunks = self.text_splitter.split_text(content)
            
            # Ø¥Ù†Ø´Ø§Ø¡ documents
            documents = [
                Document(page_content=chunk, metadata=metadata)
                for chunk in chunks
            ]
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù‚Ø§Ø¹Ø¯Ø©
            self.vectorstore.add_documents(documents)
            
            # Ø­ÙØ¸ Ø§Ù„ØªØ­Ø¯ÙŠØ«
            self.vectorstore.save_local(self.vectorstore_path)
            
            logger.info(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© {len(documents)} Ù‚Ø·Ø¹Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {e}")
            return False
    
    async def answer_with_advanced_rag(self, user_query: str) -> Dict[str, Any]:
        """Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAG (Retrieval Augmented Generation)"""
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
            relevant_docs = []
            if self.vectorstore:
                relevant_docs = self.vectorstore.similarity_search(user_query, k=3)
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚
            context = "\n".join([doc.page_content for doc in relevant_docs]) if relevant_docs else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            is_analytical_query = self._is_analytical_query(user_query)
            
            if is_analytical_query and self.claude_model:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Claude Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚
                analysis = await self._claude_analysis(context, user_query)
                
                if self.gpt_model:
                    # GPT Ù„Ù„ØµÙŠØ§ØºØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø±Ø­Ø©
                    final_response = await self._gpt_final_polish(analysis, user_query)
                else:
                    final_response = analysis
                    
                return {
                    'text': final_response,
                    'model_used': 'claude+gpt-hybrid',
                    'has_vectorstore_context': bool(relevant_docs),
                    'context_sources': len(relevant_docs)
                }
            
            elif self.gpt_model:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
                response = await self._gpt_response(context, user_query)
                
                return {
                    'text': response,
                    'model_used': 'gpt-4o-rag',
                    'has_vectorstore_context': bool(relevant_docs),
                    'context_sources': len(relevant_docs)
                }
            
            else:
                return {
                    'text': 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø®Ø¯Ù…Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.',
                    'model_used': 'none',
                    'has_vectorstore_context': False,
                    'context_sources': 0
                }
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©: {e}")
            return {
                'text': 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ.',
                'model_used': 'error',
                'has_vectorstore_context': False,
                'context_sources': 0,
                'error': str(e)
            }
    
    def _is_analytical_query(self, query: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚"""
        analytical_keywords = [
            'ØªØ­Ù„ÙŠÙ„', 'Ù†Ù‚Ø¯', 'Ø¥Ø¹Ø±Ø§Ø¨', 'Ø¨Ù„Ø§ØºØ©', 'Ø¹Ø±ÙˆØ¶',
            'Ù†Ø­Ùˆ', 'ØµØ±Ù', 'Ø£Ø³Ù„ÙˆØ¨', 'Ø¨Ù†ÙŠØ©', 'Ù†Ø¸Ø±ÙŠØ©'
        ]
        return any(keyword in query.lower() for keyword in analytical_keywords)
    
    async def _claude_analysis(self, context: str, question: str) -> str:
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Claude"""
        if not self.claude_model:
            raise ValueError("Claude model not available")
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        analysis_type = self._determine_analysis_type(question)
        
        enhanced_prompt = f"""
{self.ghassan_prompt.template}

Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {analysis_type}

Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø®Ø§ØµØ©:
- Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ù†Ù‚Ø¯ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
- Ø§Ø±Ø¨Ø· Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ
- ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª
"""

        response = self.claude_model.predict(
            enhanced_prompt.format(context=context, question=question)
        )
        return response
    
    async def _gpt_final_polish(self, analysis: str, original_question: str) -> str:
        """ØµÙŠØ§ØºØ© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø±Ø­Ø© ÙˆÙˆØ¯ÙˆØ¯Ø© Ø¨Ù€ GPT"""
        if not self.gpt_model:
            return analysis
        
        polish_prompt = f"""
Ø£Ù†Øª ØºØ³Ø§Ù† Ø§Ù„ÙˆØ¯ÙˆØ¯! Ø®Ø° Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙˆØ£Ø¹Ø¯ ØµÙŠØ§ØºØªÙ‡ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø±Ø­Ø© ÙˆÙ„Ø·ÙŠÙØ©:

Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ:
{analysis}

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ:
{original_question}

Ø§Ø¬Ø¹Ù„Ù‡:
- Ù…Ø±Ø­Ø§Ù‹ ÙˆÙˆØ¯ÙˆØ¯Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ù… "ÙŠØ§ ØµØ¯ÙŠÙ‚ÙŠ"ØŒ "Ù…Ø§ Ø£Ø±ÙˆØ¹Ùƒ!"
- Ø¨Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ø£ØµÙŠÙ„Ø©: "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ"ØŒ "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡"
- Ù…Ø­Ø§ÙØ¸Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©
- Ù…Ù†Ø¸Ù…Ø§Ù‹ ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹
- Ù…ØªØ­Ù…Ø³Ø§Ù‹ Ù„Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠ

Ø§Ø­ØªÙØ¸ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙÙ‚Ø· ØºÙŠØ± Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù„ÙŠØµØ¨Ø­ Ø£ÙƒØ«Ø± Ù…Ø±Ø­Ø§Ù‹!
"""
        
        polished = self.gpt_model.predict(polish_prompt)
        return polished
    
    async def _gpt_response(self, context: str, question: str) -> str:
        """Ø±Ø¯ Ù…Ø¨Ø§Ø´Ø± Ù…Ù† GPT Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©"""
        if not self.gpt_model:
            raise ValueError("GPT model not available")
        
        response = self.gpt_model.predict(
            self.ghassan_prompt.format(context=context, question=question)
        )
        return response
    
    def _determine_analysis_type(self, question: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        if 'Ù†Ø­Ùˆ' in question or 'Ø¥Ø¹Ø±Ø§Ø¨' in question:
            return 'ØªØ­Ù„ÙŠÙ„ Ù†Ø­ÙˆÙŠ Ù…ØªØ®ØµØµ'
        elif 'Ø¨Ù„Ø§ØºØ©' in question or 'ØµÙˆØ±Ø© Ø´Ø¹Ø±ÙŠØ©' in question:
            return 'ØªØ­Ù„ÙŠÙ„ Ø¨Ù„Ø§ØºÙŠ ÙˆØ£Ø³Ù„ÙˆØ¨ÙŠ'
        elif 'Ø¹Ø±ÙˆØ¶' in question or 'Ø¨Ø­Ø±' in question:
            return 'ØªØ­Ù„ÙŠÙ„ Ø¹Ø±ÙˆØ¶ÙŠ ÙˆØ¥ÙŠÙ‚Ø§Ø¹ÙŠ'
        elif 'Ù†Ù‚Ø¯' in question or 'Ù†Ø¸Ø±ÙŠØ©' in question:
            return 'Ù†Ù‚Ø¯ Ø£Ø¯Ø¨ÙŠ Ø¨Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©'
        else:
            return 'ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø¨ÙŠ Ø´Ø§Ù…Ù„'

    async def search_vectorstore(self, query: str, k: int = 5) -> List[Document]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©"""
        if not self.vectorstore:
            return []
        
        try:
            return self.vectorstore.similarity_search(query, k=k)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡: {e}")
            return []
    
    async def get_vectorstore_stats(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©"""
        try:
            if not self.vectorstore:
                return {"status": "ØºÙŠØ± Ù…ØªØ§Ø­", "count": 0}
            
            # Ø¹Ø¯Ø¯ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚
            index_size = self.vectorstore.index.ntotal if hasattr(self.vectorstore, 'index') else 0
            
            return {
                "status": "Ù†Ø´Ø·",
                "documents_count": index_size,
                "embeddings_available": bool(self.embeddings),
                "vectorstore_path": self.vectorstore_path
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return {"status": "Ø®Ø·Ø£", "error": str(e)}

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
advanced_ghassan_service = AdvancedGhassanService()